{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fba5d3f-d36f-4b1a-8a49-838a84219b3f",
   "metadata": {},
   "source": [
    "# ImageBind example\n",
    "\n",
    "**Environment**\n",
    "\n",
    "- Raspberry Pi5 8GB, running Raspberry Pi OS (Bookworm, 64-bit) and booting from SSD.\n",
    "- The Pi has swap size of 2048MB.\n",
    "- GUI is disabled on the Pi.\n",
    "- ImageBind model pre-downloaded to the designated/expected path (see Directory structure below).\n",
    "\n",
    "**Directory structure**\n",
    "\n",
    "```bash\n",
    "app\n",
    "├── .packages                   \n",
    "├── ImageBind                   <-- cloned ImageBind repository\n",
    "│   ├── .checkpoints            <-- ImageBind looks for this directory when loading the model\n",
    "│   │   └── imagebind_huge.pth  <-- pre-downloaded ImageBind model\n",
    "│   └── ...\n",
    "└── scripts\n",
    "    └── example.ipynb           <-- this notebook\n",
    "```\n",
    "\n",
    "**References**\n",
    "- https://github.com/facebookresearch/ImageBind?tab=readme-ov-file#usage\n",
    "- https://jina.ai/news/cross-modal-search-with-imagebind-and-docarray/\n",
    "\n",
    "<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0378afcd-fdc0-467e-aaaf-06bdc85c6578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/app/ImageBind')\n",
    "\n",
    "print(os.getcwd()) # expected output: /app/ImageBind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797d065f-9622-4699-aa55-2f2e2526bb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from imagebind import data\n",
    "from imagebind.models import imagebind_model\n",
    "from imagebind.models.imagebind_model import ModalityType\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e2005-1b1a-4378-b888-eedf9a2db421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ImageBind model\n",
    "# This can take 1-2 minutes. Make sure you have 2GB swap on 8GB Pi5, otherwise this will likely crash the Pi.\n",
    "\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f9e8b1-e7e1-4fbe-a300-5c29b20f3673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "text = [\"A dog.\", \"A car.\", \"A bird.\"]\n",
    "image_paths = [\".assets/dog_image.jpg\", \".assets/car_image.jpg\", \".assets/bird_image.jpg\"]\n",
    "audio_paths = [\".assets/dog_audio.wav\", \".assets/car_audio.wav\", \".assets/bird_audio.wav\"]\n",
    "\n",
    "inputs = {\n",
    "    ModalityType.TEXT: data.load_and_transform_text(text, device),\n",
    "    ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device),\n",
    "    ModalityType.AUDIO: data.load_and_transform_audio_data(audio_paths, device),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eeb21a-5f21-4563-8c20-c91598207790",
   "metadata": {},
   "source": [
    "## Basic example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7593f3-2723-4e27-878d-711b6ebed1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = model(inputs)\n",
    "\n",
    "print(\n",
    "    \"Vision x Text: \",\n",
    "    torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.TEXT].T, dim=-1),\n",
    ")\n",
    "print(\n",
    "    \"Audio x Text: \",\n",
    "    torch.softmax(embeddings[ModalityType.AUDIO] @ embeddings[ModalityType.TEXT].T, dim=-1),\n",
    ")\n",
    "print(\n",
    "    \"Vision x Audio: \",\n",
    "    torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.AUDIO].T, dim=-1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ff3d61-1722-4725-aadc-47c021ab32b6",
   "metadata": {},
   "source": [
    "## Cross-modal search example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8504c851-f7ac-49d4-a6a7-0ccfddd479dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install docarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09addc6d-8632-4f24-928f-f1b4b0c762f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from docarray.documents import TextDoc, ImageDoc, AudioDoc\n",
    "\n",
    "def embed(doc: Union[TextDoc, ImageDoc, AudioDoc]):\n",
    "    \"\"\"inplace embedding of document\"\"\"\n",
    "    with torch.no_grad():\n",
    "        if isinstance(doc, TextDoc):\n",
    "            embedding = model({ModalityType.TEXT: data.load_and_transform_text([doc.text], device)})[ModalityType.TEXT]\n",
    "        elif isinstance(doc, ImageDoc):\n",
    "            embedding = model({ModalityType.VISION: data.load_and_transform_vision_data([doc.url], device)})[ModalityType.VISION]\n",
    "        elif isinstance(doc, AudioDoc):\n",
    "            embedding = model({ModalityType.AUDIO: data.load_and_transform_audio_data([doc.url], device)})[ModalityType.AUDIO]\n",
    "        else:\n",
    "            raise ValueError('one of the modality fields need to be set')\n",
    "\n",
    "    doc.embedding = embedding.detach().cpu().numpy()[0]\n",
    "\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1397d654-9aad-4edf-9aa5-f28e368ba6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-to-image\n",
    "\n",
    "from docarray.index.backends.in_memory import InMemoryExactNNIndex\n",
    "\n",
    "image_index = InMemoryExactNNIndex[ImageDoc]()\n",
    "image_index.index([\n",
    "    embed(doc) for doc in \n",
    "    [ImageDoc(url=path) for path in image_paths]\n",
    "])\n",
    "\n",
    "match = image_index.find(embed(TextDoc(text='bird')).embedding, search_field='embedding', limit=1).documents[0]\n",
    "match.url.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e31531a-0471-4817-aadf-3bf88efd2ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-to-audio\n",
    "\n",
    "from docarray.index.backends.in_memory import InMemoryExactNNIndex\n",
    "\n",
    "audio_index = InMemoryExactNNIndex[AudioDoc]()\n",
    "audio_index.index([\n",
    "    embed(doc) for doc in\n",
    "    [AudioDoc(url=path) for path in audio_paths]\n",
    "])\n",
    "\n",
    "match = audio_index.find(embed(TextDoc(text='bird')).embedding, search_field='embedding', limit=1).documents[0]\n",
    "match.url.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b59db9-befe-42de-acbd-a20aa8c32916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image-to-audio\n",
    "\n",
    "from docarray.index.backends.in_memory import InMemoryExactNNIndex\n",
    "\n",
    "audio_index = InMemoryExactNNIndex[AudioDoc]()\n",
    "audio_index.index([\n",
    "    embed(doc) for doc in\n",
    "    [AudioDoc(url=path) for path in audio_paths]\n",
    "])\n",
    "\n",
    "match = audio_index.find(embed(ImageDoc(url='.assets/dog_image.jpg')).embedding, search_field='embedding', limit=1).documents[0]\n",
    "match.url.display()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
